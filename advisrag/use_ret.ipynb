{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def weighted_mean_pooling(hidden, attention_mask):\n",
    "    attention_mask_ = attention_mask * attention_mask.cumsum(dim=1)\n",
    "    s = torch.sum(hidden * attention_mask_.unsqueeze(-1).float(), dim=1)\n",
    "    d = attention_mask_.sum(dim=1, keepdim=True).float()\n",
    "    reps = s / d\n",
    "    return reps\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(text_or_image_list):\n",
    "    \n",
    "    if (isinstance(text_or_image_list[0], str)):\n",
    "        inputs = {\n",
    "            \"text\": text_or_image_list,\n",
    "            'image': [None] * len(text_or_image_list),\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "    else:\n",
    "        inputs = {\n",
    "            \"text\": [''] * len(text_or_image_list),\n",
    "            'image': text_or_image_list,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "    outputs = model(**inputs)\n",
    "    attention_mask = outputs.attention_mask\n",
    "    hidden = outputs.last_hidden_state\n",
    "\n",
    "    reps = weighted_mean_pooling(hidden, attention_mask)   \n",
    "    embeddings = F.normalize(reps, p=2, dim=1).detach().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "model_name_or_path = \"./VisRAG-Ret\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "queries = [\"What does a dog look like?\"]\n",
    "INSTRUCTION = \"Represent this query for retrieving relevant documents: \"\n",
    "queries = [INSTRUCTION + query for query in queries]\n",
    "\n",
    "print(\"Downloading images...\")\n",
    "passages = [\n",
    "    Image.open(BytesIO(requests.get(\n",
    "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/cat.jpeg'\n",
    "    ).content)).convert('RGB'),\n",
    "    Image.open(BytesIO(requests.get(\n",
    "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/dog.jpg'\n",
    "    ).content)).convert('RGB')\n",
    "]\n",
    "print(\"Images downloaded.\")\n",
    "\n",
    "embeddings_query = encode(queries)\n",
    "embeddings_doc = encode(passages)\n",
    "\n",
    "scores = (embeddings_query @ embeddings_doc.T)\n",
    "print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def weighted_mean_pooling(hidden, attention_mask):\n",
    "    attention_mask_ = attention_mask * attention_mask.cumsum(dim=1)\n",
    "    s = torch.sum(hidden * attention_mask_.unsqueeze(-1).float(), dim=1)\n",
    "    d = attention_mask_.sum(dim=1, keepdim=True).float()\n",
    "    reps = s / d\n",
    "    return reps\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(text_or_image_list):\n",
    "    if isinstance(text_or_image_list[0], str):\n",
    "        inputs = {\n",
    "            \"text\": text_or_image_list,\n",
    "            'image': [None] * len(text_or_image_list),\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "    else:\n",
    "        inputs = {\n",
    "            \"text\": [''] * len(text_or_image_list),\n",
    "            'image': text_or_image_list,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "    outputs = model(**inputs)\n",
    "    attention_mask = outputs.attention_mask\n",
    "    hidden = outputs.last_hidden_state\n",
    "\n",
    "    reps = weighted_mean_pooling(hidden, attention_mask)\n",
    "    embeddings = F.normalize(reps, p=2, dim=1).detach().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "model_name_or_path = \"./VisRAG-Ret\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name_or_path, torch_dtype=torch.float32, trust_remote_code=True)  # 使用float32以兼容CPU\n",
    "# # model.eval()  # 确保模型在CPU上运行\n",
    "\n",
    "# queries = [\"What does a dog look like?\"]\n",
    "# INSTRUCTION = \"Represent this query for retrieving relevant documents: \"\n",
    "# queries = [INSTRUCTION + query for query in queries]\n",
    "\n",
    "# print(\"Downloading images...\")\n",
    "# passages = [\n",
    "#     Image.open(BytesIO(requests.get(\n",
    "#         'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/cat.jpeg'\n",
    "#     ).content)).convert('RGB'),\n",
    "#     Image.open(BytesIO(requests.get(\n",
    "#         'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/dog.jpg'\n",
    "#     ).content)).convert('RGB')\n",
    "# ]\n",
    "# print(\"Images downloaded.\")\n",
    "\n",
    "# embeddings_query = encode(queries)\n",
    "# embeddings_doc = encode(passages)\n",
    "# print(encode([]))\n",
    "# scores = (embeddings_query @ embeddings_doc.T)\n",
    "# print(scores.tolist())\n",
    "\n",
    "# # 计算余弦相似度\n",
    "# cosine_sim = F.cosine_similarity(torch.tensor(embeddings_query), torch.tensor(embeddings_doc), dim=-1)\n",
    "# print(cosine_sim.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2508280873298645, 0.34044021368026733]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# 下载并打开图像\n",
    "passages = [\n",
    "    Image.open(BytesIO(requests.get(\n",
    "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/cat.jpeg'\n",
    "    ).content)).convert('RGB'),\n",
    "    Image.open(BytesIO(requests.get(\n",
    "        'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/dog.jpg'\n",
    "    ).content)).convert('RGB')\n",
    "]\n",
    "\n",
    "# 对应的summary\n",
    "summaries = [\n",
    "    \"This is a cat.\",\n",
    "    \"This is a dog.\"\n",
    "]\n",
    "\n",
    "# 在图像下方添加文字\n",
    "def add_text_below_image(image, text):\n",
    "    # 创建一个新的图像，包含原图像和文字区域\n",
    "    font = ImageFont.load_default()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # 计算文字区域的高度\n",
    "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "    padding = 10  # 文字区域的上下边距\n",
    "    total_text_height = text_height + 2 * padding\n",
    "    \n",
    "    new_image = Image.new('RGB', (image.width, image.height + total_text_height), (255, 255, 255))\n",
    "    \n",
    "    # 将原图像粘贴到新图像上\n",
    "    new_image.paste(image, (0, 0))\n",
    "    \n",
    "    # 在新图像的下方添加文字\n",
    "    draw = ImageDraw.Draw(new_image)\n",
    "    text_position = (10, image.height + padding)  # 文字位置，可以根据需要调整\n",
    "    draw.text(text_position, text, font=font, fill=\"black\")\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "# 处理每个图像并添加summary\n",
    "processed_images = []\n",
    "for image, summary in zip(passages, summaries):\n",
    "    processed_image = add_text_below_image(image, summary)\n",
    "    processed_images.append(processed_image)\n",
    "\n",
    "# # 显示处理后的图像\n",
    "# for img in processed_images:\n",
    "#     img.show()\n",
    "    \n",
    "# for i, img in enumerate(processed_images):\n",
    "#     img.save(f\"processed_image_{i}.jpg\")\n",
    "\n",
    "\n",
    "queries = [\"What does a dog look like?\"]\n",
    "INSTRUCTION = \"Represent this query for retrieving relevant documents: \"\n",
    "queries = [INSTRUCTION + query for query in queries]\n",
    "\n",
    "# print(\"Downloading images...\")\n",
    "# passages = [\n",
    "#     Image.open(BytesIO(requests.get(\n",
    "#         'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/cat.jpeg'\n",
    "#     ).content)).convert('RGB'),\n",
    "#     Image.open(BytesIO(requests.get(\n",
    "#         'https://github.com/OpenBMB/VisRAG/raw/refs/heads/master/scripts/demo/retriever/test_image/dog.jpg'\n",
    "#     ).content)).convert('RGB')\n",
    "# ]\n",
    "# print(\"Images downloaded.\")\n",
    "\n",
    "embeddings_query = encode(queries)\n",
    "embeddings_doc = encode(processed_images)\n",
    "\n",
    "scores = (embeddings_query @ embeddings_doc.T)\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load datasets\n",
    "MP_DocVQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-MP-DocVQA\", name=\"corpus\", split=\"train\")\n",
    "MP_DocVQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-MP-DocVQA\", name=\"queries\", split=\"train\")\n",
    "\n",
    "ArxivQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-ArxivQA\", name=\"corpus\", split=\"train\")\n",
    "ArxivQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-ArxivQA\", name=\"queries\", split=\"train\")\n",
    "\n",
    "ChartQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-ChartQA\", name=\"corpus\", split=\"train\")\n",
    "ChartQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-ChartQA\", name=\"queries\", split=\"train\")\n",
    "\n",
    "InfoVQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-InfoVQA\", name=\"corpus\", split=\"train\")\n",
    "InfoVQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-InfoVQA\", name=\"queries\", split=\"train\")\n",
    "\n",
    "PlotQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-PlotQA\", name=\"corpus\", split=\"train\")\n",
    "PlotQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-PlotQA\", name=\"queries\", split=\"train\")\n",
    "\n",
    "SlideVQA_corpus_ds = load_dataset(\"dataset/VisRAG-Ret-Test-SlideVQA\", name=\"corpus\", split=\"train\")\n",
    "SlideVQA_queries_ds = load_dataset(\"dataset/VisRAG-Ret-Test-SlideVQA\", name=\"queries\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "target_query_id = \"2012-02-20fy11roadshow-120221022442-phpapp02_95__feb-20-2012-nestl-2011-fullyear-roadshow-presentation-5-1024.jpgquery_number_1\"\n",
    "target_doc_id = \"analysisofkoreanwinemarket-20150902-daejeon-150829090424-lva1-app6891_95__analysis-of-korean-wine-market-20150902daejeon-19-1024.jpg\"\n",
    "\n",
    "query_em = None\n",
    "for query in SlideVQA_queries_ds:\n",
    "    if query['query-id'] == target_query_id:\n",
    "        print(query['query'])\n",
    "        query_em = encode([query['query']])\n",
    "        break\n",
    "\n",
    "doc_em = None\n",
    "for doc in SlideVQA_corpus_ds:\n",
    "    if doc['corpus-id'] == target_doc_id:\n",
    "        image = doc['image']\n",
    "        # 显示图片\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')  # 隐藏坐标轴\n",
    "        plt.show()\n",
    "        doc_em = encode([doc['image'].convert('RGB')])\n",
    "        break\n",
    "\n",
    "\n",
    "sim = np.dot(query_em, doc_em.T)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def encode_and_save_embeddings(dataset, dataset_name):\n",
    "    image_embeddings = []\n",
    "    corpus_ids = []\n",
    "    total_examples = len(dataset)  \n",
    "\n",
    "    for i, example in enumerate(dataset):  \n",
    "        image = example['image'].convert('RGB')\n",
    "        embedding = encode([image])\n",
    "        image_embeddings.append(embedding)\n",
    "        corpus_ids.append(example['corpus-id'])\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_examples:  # 每处理10个样本或最后一个样本时输出进度  \n",
    "            progress = (i + 1) / total_examples * 100  \n",
    "            print(f\"Processing {dataset_name}: {i + 1}/{total_examples} ({progress:.2f}%)\")  \n",
    "\n",
    "    # 将嵌入列表转换为numpy数组\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "\n",
    "    # 保存嵌入和corpus-id到文件\n",
    "    np.save(f\"embeddings/{dataset_name}_embeddings.npy\", image_embeddings)\n",
    "    np.save(f\"embeddings/{dataset_name}_corpus_ids.npy\", np.array(corpus_ids))\n",
    "\n",
    "encode_and_save_embeddings(MP_DocVQA_corpus_ds, \"MP_DocVQA_corpus\")\n",
    "encode_and_save_embeddings(SlideVQA_corpus_ds, \"SlideVQA_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def encode_and_save_summary_embeddings(summary_path, dataset, dataset_name):\n",
    "    with open(summary_path, 'r', encoding='utf-8') as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    image_summary_embeddings = []\n",
    "    total_examples = len(dataset)\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        corpus_id = example['corpus-id']\n",
    "        summary = summaries.get(corpus_id, \"\")\n",
    "        \n",
    "        embedding = encode([summary])\n",
    "        image_summary_embeddings.append(embedding)\n",
    "        \n",
    "        # 输出进度\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_examples:\n",
    "            progress = (i + 1) / total_examples * 100\n",
    "            print(f\"Processing {dataset_name}: {i + 1}/{total_examples} ({progress:.2f}%)\")\n",
    "    \n",
    "    image_summary_embeddings = np.vstack(image_summary_embeddings)\n",
    "    np.save(f\"embeddings/{dataset_name}_summary_embeddings.npy\", image_summary_embeddings)\n",
    "    \n",
    "# encode_and_save_summary_embeddings(\"./MP_DocVQA_summary.jsonl\", MP_DocVQA_corpus_ds, \"MP_DocVQA_corpus\")\n",
    "encode_and_save_summary_embeddings(\"./SlideVQA_summary.jsonl\", SlideVQA_corpus_ds, \"SlideVQA_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:45<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode_and_save_sparse_ret_embeddings(summary_path, dataset, dataset_name):\n",
    "    with open(summary_path, 'r', encoding='utf-8') as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    image_summary_embeddings = []\n",
    "    # total_examples = len(dataset)\n",
    "    \n",
    "    for example in tqdm(dataset):\n",
    "        corpus_id = example['corpus-id']\n",
    "        summary = summaries.get(corpus_id, \"\")\n",
    "        # title = summary['Title']\n",
    "        # keywords = \", \".join(summary['Keywords'])\n",
    "        # keywords = summary['Keywords']\n",
    "        description = summary['Description']\n",
    "        # image_type = summary['Image Type']\n",
    "        \n",
    "        \n",
    "        embedding = encode([description])\n",
    "        # embedding = encode(list(summary.values()))\n",
    "        # embedding = encode([title, description, image_type] + keywords)\n",
    "        # sum_array = np.sum(embedding, axis=0)\n",
    "        image_summary_embeddings.append(embedding)\n",
    "        \n",
    "        # 输出进度\n",
    "        # if (i + 1) % 10 == 0 or (i + 1) == total_examples:\n",
    "        #     progress = (i + 1) / total_examples * 100\n",
    "        #     print(f\"Processing {dataset_name}: {i + 1}/{total_examples} ({progress:.2f}%)\")\n",
    "    \n",
    "    image_summary_embeddings = np.vstack(image_summary_embeddings)\n",
    "    np.save(f\"embeddings/{dataset_name}_summary_embeddings.npy\", image_summary_embeddings)\n",
    "    \n",
    "# encode_and_save_summary_embeddings(\"./MP_DocVQA_summary.jsonl\", MP_DocVQA_corpus_ds, \"MP_DocVQA_corpus\")\n",
    "encode_and_save_sparse_ret_embeddings(\"./ChartQA_image_keywords.json\", ChartQA_corpus_ds, \"ChartQA_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/09/2025 22:37:10 - INFO - __main__ -   Evaluating PlotQA dataset\n",
      "01/09/2025 22:37:34 - INFO - __main__ -   ndcg_cut_10              all     0.4446\n",
      "01/09/2025 22:37:34 - INFO - __main__ -   recall_10                all     0.6031\n",
      "01/09/2025 22:37:34 - INFO - __main__ -   MRR@10: 0.39496824554532173\n",
      "01/09/2025 22:37:34 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    # {\n",
    "    #     \"name\": \"SlideVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ArxivQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ChartQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_summary_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"InfoVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        # \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        # \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings_grayscale.npy\",\n",
    "        # \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_image_plus_summary_embeddings.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_image_plus_summary_plus_title_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_save_query_embeddings(dataset, dataset_name):  \n",
    "    query_embeddings = []\n",
    "    query_ids = []  \n",
    "    total_examples = len(dataset)  \n",
    "  \n",
    "    for i, example in enumerate(dataset):  \n",
    "        query = example['query']\n",
    "        embedding = encode([query])  \n",
    "        query_embeddings.append(embedding)\n",
    "        query_ids.append(example['query-id'])  \n",
    "  \n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_examples:  # 每处理10个样本或最后一个样本时输出进度  \n",
    "            progress = (i + 1) / total_examples * 100  \n",
    "            print(f\"Processing {dataset_name}: {i + 1}/{total_examples} ({progress:.2f}%)\")  \n",
    "  \n",
    "    # 将嵌入列表转换为numpy数组\n",
    "    query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "    # 保存嵌入和corpus-id到文件\n",
    "    np.save(f\"embeddings/{dataset_name}_embeddings.npy\", query_embeddings)\n",
    "    np.save(f\"embeddings/{dataset_name}_query_ids.npy\", np.array(query_ids))\n",
    "  \n",
    "encode_and_save_query_embeddings(MP_DocVQA_queries_ds, \"MP_DocVQA_queries\")  \n",
    "encode_and_save_query_embeddings(SlideVQA_queries_ds, \"SlideVQA_queries\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_save_query_with_instruction_embeddings(dataset, dataset_name):  \n",
    "    INSTRUCTION = \"Represent this query for retrieving relevant documents: \"\n",
    "    query_embeddings = []\n",
    "    # query_ids = []  \n",
    "    total_examples = len(dataset)  \n",
    "  \n",
    "    for i, example in enumerate(dataset):  \n",
    "        query = INSTRUCTION + example['query']\n",
    "        embedding = encode([query])  \n",
    "        query_embeddings.append(embedding)\n",
    "        # query_ids.append(example['query-id'])  \n",
    "  \n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_examples:  # 每处理10个样本或最后一个样本时输出进度  \n",
    "            progress = (i + 1) / total_examples * 100  \n",
    "            print(f\"Processing {dataset_name}: {i + 1}/{total_examples} ({progress:.2f}%)\")  \n",
    "  \n",
    "    # 将嵌入列表转换为numpy数组\n",
    "    query_embeddings = np.vstack(query_embeddings)\n",
    "\n",
    "    # 保存嵌入和corpus-id到文件\n",
    "    np.save(f\"embeddings/{dataset_name}_with_instruction_embeddings.npy\", query_embeddings)\n",
    "    # np.save(f\"embeddings/{dataset_name}_query_ids.npy\", np.array(query_ids))\n",
    "  \n",
    "encode_and_save_query_with_instruction_embeddings(MP_DocVQA_queries_ds, \"MP_DocVQA_queries\")  \n",
    "encode_and_save_query_with_instruction_embeddings(SlideVQA_queries_ds, \"SlideVQA_queries\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/17/2024 15:01:28 - INFO - __main__ -   Evaluating PlotQA dataset\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   ndcg_cut_10              all     0.4524\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   recall_10                all     0.6140\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   MRR@10: 0.4018738567624206\n",
      "12/17/2024 15:01:31 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    # {\n",
    "    #     \"name\": \"SlideVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ArxivQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ChartQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"InfoVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/17/2024 18:07:30 - INFO - __main__ -   Evaluating ChartQA dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/17/2024 18:07:30 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/17/2024 18:07:30 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/17/2024 18:07:30 - INFO - __main__ -   ndcg_cut_10              all     0.6204\n",
      "12/17/2024 18:07:30 - INFO - __main__ -   recall_10                all     0.7242\n",
      "12/17/2024 18:07:30 - INFO - __main__ -   MRR@10: 0.5873922270858202\n",
      "12/17/2024 18:07:30 - INFO - __main__ -   \n",
      "12/17/2024 18:07:30 - INFO - __main__ -   Evaluating PlotQA dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ChartQA_results_with_score.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/17/2024 18:07:33 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/17/2024 18:07:33 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/17/2024 18:07:33 - INFO - __main__ -   ndcg_cut_10              all     0.4524\n",
      "12/17/2024 18:07:33 - INFO - __main__ -   recall_10                all     0.6140\n",
      "12/17/2024 18:07:33 - INFO - __main__ -   MRR@10: 0.4018738567624206\n",
      "12/17/2024 18:07:33 - INFO - __main__ -   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to PlotQA_results_with_score.json\n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "import json\n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def save_result_json(run, qrels, dataset_name):\n",
    "    results = []\n",
    "    for query, retrieved in run.items():\n",
    "        result = {\n",
    "            \"query_id\": query,\n",
    "            \"true_image_id\": list(qrels.get(query, {}).keys()), \n",
    "            # \"retrive_image_id\": list(retrieved.keys())\n",
    "            \"retrive_image_id\": retrieved\n",
    "        }\n",
    "        results.append(result)\n",
    "        # for retrive_image_id, score in retrieved.items():\n",
    "        #     result = {\n",
    "        #         \"query\": query,\n",
    "        #         \"true_image_id\": qrels.get(query, \"unknown\"),\n",
    "        #         \"retrive_image_id\": retrive_image_id\n",
    "        #     }\n",
    "        #     results.append(result)\n",
    "    \n",
    "    with open(f\"{dataset_name}_results_with_score.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {dataset_name}_results_with_score.json\")\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels, dataset_name):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        \n",
    "        save_result_json(run, qrels, dataset_name)\n",
    "        \n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "        \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    # {\n",
    "    #     \"name\": \"SlideVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ArxivQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"ChartQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"InfoVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels, dataset['name'])\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/30/2024 18:52:02 - INFO - __main__ -   Evaluating PlotQA dataset\n",
      "12/30/2024 18:52:29 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/30/2024 18:52:29 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/30/2024 18:52:29 - INFO - __main__ -   ndcg_cut_10              all     0.4477\n",
      "12/30/2024 18:52:29 - INFO - __main__ -   recall_10                all     0.6061\n",
      "12/30/2024 18:52:30 - INFO - __main__ -   MRR@10: 0.3982066524319113\n",
      "12/30/2024 18:52:30 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    # {\n",
    "    #     \"name\": \"SlideVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_image_plus_summary_embeddings_2.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ArxivQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"ChartQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_image_plus_summary_embeddings_1.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"InfoVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_image_plus_summary_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2024 15:34:45 - INFO - __main__ -   Evaluating SlideVQA dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2024 15:34:45 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   ndcg_cut_11              all     0.9152\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   recall_11                all     0.9704\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   MRR@11: 0.9176814970260092\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   \n",
      "12/14/2024 15:34:45 - INFO - __main__ -   Evaluating MP_DocVQA dataset\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   ndcg_cut_11              all     0.8144\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   recall_11                all     0.9329\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   MRR@11: 0.7765095069911457\n",
      "12/14/2024 15:34:45 - INFO - __main__ -   \n",
      "12/14/2024 15:34:45 - INFO - __main__ -   Evaluating ArxivQA dataset\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   ndcg_cut_11              all     0.7018\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   recall_11                all     0.8108\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   MRR@11: 0.6675316074100784\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   \n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Evaluating ChartQA dataset\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   ndcg_cut_11              all     0.6227\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   recall_11                all     0.7326\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   MRR@11: 0.5881519130822749\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   \n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Evaluating InfoVQA dataset\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   ndcg_cut_11              all     0.8721\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   recall_11                all     0.9663\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   MRR@11: 0.8415744866624622\n",
      "12/14/2024 15:34:47 - INFO - __main__ -   \n",
      "12/14/2024 15:34:47 - INFO - __main__ -   Evaluating PlotQA dataset\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   ndcg_cut_11              all     0.4548\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   recall_11                all     0.6226\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   MRR@11: 0.40265374371901175\n",
      "12/14/2024 15:34:50 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:11]  # 取前5个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.11\", \"recall.11\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_11 = eval_mrr(qrels, run, 11)['all']  \n",
    "        logger.info(f'MRR@11: {mrr_at_11}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "# datasets = [\n",
    "#     {\n",
    "#         \"name\": \"SlideVQA\",\n",
    "#         \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "#         \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "#         \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "#         \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "#         \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"MP_DocVQA\",\n",
    "#         \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "#         \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "#         \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "#         \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "#         \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MP_DocVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ArxivQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ChartQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"InfoVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def eval_mrr_with_wrong_cases(qrel, run, cutoff=None):\n",
    "    \"\"\"\n",
    "    计算MRR@cutoff并识别错误案例。\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    num_ranked_q = 0\n",
    "    results = {}\n",
    "    wrong_cases = []\n",
    "\n",
    "    for qid in qrel:\n",
    "        if qid not in run:\n",
    "            continue\n",
    "        num_ranked_q += 1\n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]\n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)\n",
    "        rr = 0.0\n",
    "        for i, (docid, _) in enumerate(docid_and_score):\n",
    "            if cutoff is None or i < cutoff:\n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:\n",
    "                    rr = 1.0 / (i + 1)\n",
    "                    break\n",
    "        if rr == 0.0:\n",
    "            wrong_cases.append(qid)\n",
    "        results[qid] = rr\n",
    "        mrr += rr\n",
    "\n",
    "    mrr /= num_ranked_q\n",
    "    results[\"all\"] = mrr\n",
    "\n",
    "    return results, wrong_cases\n",
    "\n",
    "def retrieve_and_evaluate_with_wrong_cases(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):\n",
    "    try:\n",
    "        run = {}\n",
    "        for q_idx, q_emb in enumerate(query_embeddings):\n",
    "            qid = query_ids[q_idx]\n",
    "            scores = np.dot(corpus_embeddings, q_emb)\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个\n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}\n",
    "            \n",
    "        # if check_dictionaries(qrels, run):\n",
    "        #     logger.info(\"Proceeding with evaluation.\")\n",
    "        # else:\n",
    "        #     logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        \n",
    "        # 评估\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})\n",
    "        eval_results = evaluator.evaluate(run)\n",
    "\n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):\n",
    "            value = pytrec_eval.compute_aggregated_measure(\n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]\n",
    "            )\n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")\n",
    "\n",
    "        mrr_at_10_results, wrong_cases = eval_mrr_with_wrong_cases(qrels, run, 10)\n",
    "        mrr_at_10 = mrr_at_10_results['all']\n",
    "        \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')\n",
    "        \n",
    "        if wrong_cases:\n",
    "            # logger.info(f'Queries with the lowest MRR@10 scores (wrong cases): {wrong_cases}')\n",
    "            for wrong in wrong_cases:\n",
    "                # logger.info(f'Query ID: {query}')\n",
    "                # logger.info(f'Expected Documents: {qrels[query]}')\n",
    "                # logger.info(f'Retrieved Documents: {run[query]}')\n",
    "                # print(f'Query: {query}')\n",
    "                for query in SlideVQA_queries_ds:\n",
    "                    if query['query-id'] == wrong:\n",
    "                        print(f\"Query: {query['query']}\")\n",
    "                        break\n",
    "                # print(f'Expected Documents: {qrels[wrong]}')\n",
    "                print(\"Expected Images:\")\n",
    "                for doc_id in qrels[wrong]:\n",
    "                    for doc in SlideVQA_corpus_ds:\n",
    "                        if doc['corpus-id'] == doc_id:\n",
    "                            image = doc['image']\n",
    "                            plt.imshow(image)\n",
    "                            plt.axis('off')  # 隐藏坐标轴\n",
    "                            plt.title(f'Expected Image ID: {doc_id}')\n",
    "                            plt.show()\n",
    "                            break\n",
    "                # print(f'Retrieved Documents: {run[wrong]}')\n",
    "                print(\"Retrieved Images: (with top 3 similarity)\")\n",
    "                # for doc_id in run[wrong]:\n",
    "                #     for doc in SlideVQA_corpus_ds:\n",
    "                #         if doc['corpus-id'] == doc_id:\n",
    "                #             image = doc['image']\n",
    "                #             plt.imshow(image)\n",
    "                #             plt.axis('off')  # 隐藏坐标轴\n",
    "                #             plt.title(f'Retrieved Image ID: {doc_id}')\n",
    "                #             plt.show()\n",
    "                #             break\n",
    "                sorted_retrieved_docs = sorted(run[wrong].items(), key=lambda item: item[1], reverse=True)\n",
    "                for doc_id, score in sorted_retrieved_docs[:3]:\n",
    "                    for doc in SlideVQA_corpus_ds:\n",
    "                        if doc['corpus-id'] == doc_id:\n",
    "                            image = doc['image']\n",
    "                            plt.imshow(image)\n",
    "                            plt.axis('off')  # 隐藏坐标轴\n",
    "                            plt.title(f'Retrieved Image ID: {doc_id}, Score: {score}')\n",
    "                            plt.show()\n",
    "                            break\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")\n",
    "       \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    "  \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MP_DocVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 示例使用提供的数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate_with_wrong_cases(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/15/2024 15:36:35 - INFO - __main__ -   Evaluating SlideVQA dataset\n",
      "12/15/2024 15:36:36 - INFO - __main__ -   ndcg_cut_10              all     0.6494\n",
      "12/15/2024 15:36:36 - INFO - __main__ -   recall_10                all     0.7728\n",
      "12/15/2024 15:36:36 - INFO - __main__ -   MRR@10: 0.6361631339527697\n",
      "12/15/2024 15:36:36 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "# use corpus summary to retrieve\n",
    "\n",
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        # \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\", # MRR@10: 0.7713586430507162\n",
    "        # \"query_embeddings_path\": \"embeddings/SlideVQA_queries_embeddings.npy\", # MRR@10: 0.6361631339527697\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_embeddings_use_baai_llm_embedder.npy\", # MRR@10: 0.7897086720867214\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_summary_embeddings.npy\",\n",
    "        # \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_summary_embeddings_use_baai_llm_embedder.npy\", \n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_summary_embeddings.npy.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/12/2024 07:38:28 - INFO - __main__ -   Evaluating SlideVQA dataset\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   ndcg_cut_10              all     0.9164\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   recall_10                all     0.9703\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   MRR@10: 0.9202281746031743\n",
      "12/12/2024 07:38:28 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "# use corpus image + summary to retrieve\n",
    "\n",
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:10]  # 取前10个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_image_plus_summary_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"MP_DocVQA\",\n",
    "    #     \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "    #     \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "    #     \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_summary_embeddings.npy.npy\",\n",
    "    #     \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "    #     \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2024 08:50:21 - INFO - __main__ -   Evaluating SlideVQA dataset\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   ndcg_cut_1               all     0.8756\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   recall_1                 all     0.7376\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   MRR@1: 0.875609756097561\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   \n",
      "12/11/2024 08:50:21 - INFO - __main__ -   Evaluating MP_DocVQA dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2024 08:50:21 - INFO - __main__ -   Both qrels and run dictionaries are correctly formatted.\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   Proceeding with evaluation.\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   ndcg_cut_1               all     0.6908\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   recall_1                 all     0.6908\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   MRR@1: 0.690792974986695\n",
      "12/11/2024 08:50:21 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "def check_dictionaries(qrels, run):\n",
    "    # 检查 qrels 字典\n",
    "    for qid, doc_scores in qrels.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Qrels for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, int):\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not an integer.\")\n",
    "                return False\n",
    "\n",
    "    # 检查 run 字典\n",
    "    for qid, doc_scores in run.items():\n",
    "        if not isinstance(doc_scores, dict):\n",
    "            logger.error(f\"Run for query {qid} is not a dictionary.\")\n",
    "            return False\n",
    "        for docid, score in doc_scores.items():\n",
    "            if not isinstance(score, (int, float)):\n",
    "                logger.info(f\"Query ID: {qid}, Doc ID: {docid}, Score: {score}, Type: {type(score)}\")\n",
    "                logger.error(f\"Score for doc {docid} in query {qid} is not a number.\")\n",
    "                return False\n",
    "\n",
    "    logger.info(\"Both qrels and run dictionaries are correctly formatted.\")\n",
    "    return True\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            # scores = np.array([cosine_similarity(q_emb, c_emb) for c_emb in corpus_embeddings])\n",
    "            top_k_indices = np.argsort(scores)[::-1][:1]  # 取前1个  \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        if check_dictionaries(qrels, run):\n",
    "            logger.info(\"Proceeding with evaluation.\")\n",
    "        else:\n",
    "            logger.error(\"Dictionary format error. Aborting evaluation.\")\n",
    "        # 评估  \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.1\", \"recall.1\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_1 = eval_mrr(qrels, run, 1)['all']  \n",
    "        logger.info(f'MRR@1: {mrr_at_1}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MP_DocVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40375298"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2012-02-20fy11roadshow-120221022442-phpapp02_95__feb-20-2012-nestl-2011-fullyear-roadshow-presentation-5-1024.jpgquery_number_1\n",
    "\n",
    "# analysisofkoreanwinemarket-20150902-daejeon-150829090424-lva1-app6891_95__analysis-of-korean-wine-market-20150902daejeon-19-1024.jpg\n",
    "\n",
    "target_query_id = \"2012-02-20fy11roadshow-120221022442-phpapp02_95__feb-20-2012-nestl-2011-fullyear-roadshow-presentation-5-1024.jpgquery_number_1\"\n",
    "q_position = np.where(query_ids == target_query_id)[0]\n",
    "\n",
    "target_doc_id = \"analysisofkoreanwinemarket-20150902-daejeon-150829090424-lva1-app6891_95__analysis-of-korean-wine-market-20150902daejeon-19-1024.jpg\"\n",
    "d_position = np.where(corpus_ids == target_doc_id)[0]\n",
    "\n",
    "q_position[0], d_position[0]\n",
    "\n",
    "dot_product = np.dot(query_embeddings[q_position[0]], corpus_embeddings[d_position[0]])\n",
    "\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2025 12:29:56 - INFO - __main__ -   Evaluating SlideVQA dataset\n",
      "01/06/2025 12:30:42 - INFO - __main__ -   ndcg_cut_10              all     0.9146\n",
      "01/06/2025 12:30:42 - INFO - __main__ -   recall_10                all     0.9686\n",
      "01/06/2025 12:30:42 - INFO - __main__ -   MRR@10: 0.9176260646535037\n",
      "01/06/2025 12:30:42 - INFO - __main__ -   \n",
      "01/06/2025 12:30:42 - INFO - __main__ -   Evaluating MP_DocVQA dataset\n",
      "01/06/2025 12:31:31 - INFO - __main__ -   ndcg_cut_10              all     0.8133\n",
      "01/06/2025 12:31:31 - INFO - __main__ -   recall_10                all     0.9292\n",
      "01/06/2025 12:31:31 - INFO - __main__ -   MRR@10: 0.7767030335284723\n",
      "01/06/2025 12:31:31 - INFO - __main__ -   \n",
      "01/06/2025 12:31:31 - INFO - __main__ -   Evaluating ArxivQA dataset\n",
      "01/06/2025 12:36:37 - INFO - __main__ -   ndcg_cut_10              all     0.7000\n",
      "01/06/2025 12:36:37 - INFO - __main__ -   recall_10                all     0.8042\n",
      "01/06/2025 12:36:37 - INFO - __main__ -   MRR@10: 0.6669318599353316\n",
      "01/06/2025 12:36:37 - INFO - __main__ -   \n",
      "01/06/2025 12:36:37 - INFO - __main__ -   Evaluating ChartQA dataset\n",
      "01/06/2025 12:36:54 - INFO - __main__ -   ndcg_cut_10              all     0.6204\n",
      "01/06/2025 12:36:54 - INFO - __main__ -   recall_10                all     0.7242\n",
      "01/06/2025 12:36:54 - INFO - __main__ -   MRR@10: 0.5873922270858202\n",
      "01/06/2025 12:36:54 - INFO - __main__ -   \n",
      "01/06/2025 12:36:54 - INFO - __main__ -   Evaluating InfoVQA dataset\n",
      "01/06/2025 12:37:46 - INFO - __main__ -   ndcg_cut_10              all     0.8711\n",
      "01/06/2025 12:37:46 - INFO - __main__ -   recall_10                all     0.9624\n",
      "01/06/2025 12:37:46 - INFO - __main__ -   MRR@10: 0.8412190258964445\n",
      "01/06/2025 12:37:46 - INFO - __main__ -   \n",
      "01/06/2025 12:37:46 - INFO - __main__ -   Evaluating PlotQA dataset\n",
      "01/06/2025 12:44:34 - INFO - __main__ -   ndcg_cut_10              all     0.4524\n",
      "01/06/2025 12:44:34 - INFO - __main__ -   recall_10                all     0.6140\n",
      "01/06/2025 12:44:34 - INFO - __main__ -   MRR@10: 0.4018738567624206\n",
      "01/06/2025 12:44:34 - INFO - __main__ -   \n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "import pytrec_eval  \n",
    "import logging  \n",
    "import numpy as np  \n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "  \n",
    "logging.basicConfig(  \n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",  \n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  \n",
    "    level=logging.INFO  \n",
    ")  \n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "  \n",
    "def eval_mrr(qrel, run, cutoff=None):  \n",
    "    \"\"\"  \n",
    "    Compute MRR@cutoff manually.  \n",
    "    \"\"\"  \n",
    "    mrr = 0.0  \n",
    "    num_ranked_q = 0  \n",
    "    results = {}  \n",
    "    for qid in qrel:  \n",
    "        if qid not in run:  \n",
    "            continue  \n",
    "        num_ranked_q += 1  \n",
    "        docid_and_score = [(docid, score) for docid, score in run[qid].items()]  \n",
    "        docid_and_score.sort(key=lambda x: x[1], reverse=True)  \n",
    "        for i, (docid, _) in enumerate(docid_and_score):  \n",
    "            rr = 0.0  \n",
    "            if cutoff is None or i < cutoff:  \n",
    "                if docid in qrel[qid] and qrel[qid][docid] > 0:  \n",
    "                    rr = 1.0 / (i + 1)  \n",
    "                    break  \n",
    "        results[qid] = rr  \n",
    "        mrr += rr  \n",
    "    mrr /= num_ranked_q  \n",
    "    results[\"all\"] = mrr  \n",
    "    return results  \n",
    "\n",
    "def view_result(dataset_name, run, qrels, corpus_embeddings, corpus_ids):\n",
    "    # good_results = defaultdict(dict)\n",
    "    good_results = defaultdict(lambda: {'gt': {}, 'run': {}})\n",
    "    bad_results = defaultdict(lambda: {'gt': {}, 'run': {}})\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for qid in run:\n",
    "        # print(f\"Query ID: {qid}\")\n",
    "        # print(f\"Expected Documents: {qrels[qid]}\")\n",
    "        # print(f\"Retrieved Documents: {run[qid]}\")\n",
    "        \n",
    "        if list(run[qid].keys())[0] not in list(qrels[qid].keys()) and i < 20:\n",
    "            for image_id in run[qid]:\n",
    "                for image, emb in zip(corpus_ids, corpus_embeddings):\n",
    "                    if image == image_id:\n",
    "                        bad_results[qid]['run'][image] = emb.tolist()\n",
    "                        break\n",
    "            for image_id in qrels[qid].keys():\n",
    "                for image, emb in zip(corpus_ids, corpus_embeddings):\n",
    "                    if image == image_id:\n",
    "                        bad_results[qid]['gt'][image] = emb.tolist()\n",
    "                        break\n",
    "            # i += 1\n",
    "        elif j < 20:\n",
    "            for image_id in run[qid]:\n",
    "                for image, emb in zip(corpus_ids, corpus_embeddings):\n",
    "                    if image == image_id:\n",
    "                        good_results[qid]['run'][image] = emb.tolist()\n",
    "                        break\n",
    "            for image_id in qrels[qid].keys():\n",
    "                for image, emb in zip(corpus_ids, corpus_embeddings):\n",
    "                    if image == image_id:\n",
    "                        good_results[qid]['gt'][image] = emb.tolist()\n",
    "                        break\n",
    "            # j += 1\n",
    "        if i >= 20 and j >= 20:\n",
    "            break\n",
    "    os.makedirs(os.path.dirname('tmp/view_result/'), exist_ok=True)\n",
    "    with open(f'tmp/view_result/view_result_{dataset_name}_all.json', 'w') as f:\n",
    "        json.dump({'good_results': good_results, 'bad_results': bad_results}, f)\n",
    "    \n",
    "    # good_resulttt = np.load(f'tmp/view_result/good_results_{dataset_name}.npy')\n",
    "    # bad_resulttt = np.load(f'tmp/view_result/bad_results_{dataset_name}.npy')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def retrieve_and_evaluate(dataset_name, query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels):  \n",
    "    try:  \n",
    "        run = {}  \n",
    "        for q_idx, q_emb in enumerate(query_embeddings):  \n",
    "            qid = query_ids[q_idx]  \n",
    "            scores = np.dot(corpus_embeddings, q_emb)  \n",
    "            top_k_indices = np.argsort(scores)[::-1][:10] \n",
    "            run[qid] = {corpus_ids[idx]: float(scores[idx]) for idx in top_k_indices}  \n",
    "            \n",
    "        view_result(dataset_name, run, qrels, corpus_embeddings, corpus_ids)\n",
    "            \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\", \"recall.10\"})  \n",
    "        eval_results = evaluator.evaluate(run)  \n",
    "  \n",
    "        for measure in sorted(eval_results[next(iter(eval_results))].keys()):  \n",
    "            value = pytrec_eval.compute_aggregated_measure(  \n",
    "                measure, [query_measures[measure] for query_measures in eval_results.values()]  \n",
    "            )  \n",
    "            logger.info(f\"{measure:25s}{'all':8s}{value:.4f}\")  \n",
    "  \n",
    "        mrr_at_10 = eval_mrr(qrels, run, 10)['all']  \n",
    "        logger.info(f'MRR@10: {mrr_at_10}')  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error during retrieval and evaluation: {e}\")  \n",
    "  \n",
    "def load_beir_qrels(qrels_file):  \n",
    "    qrels = {}  \n",
    "    try:  \n",
    "        with open(qrels_file) as f:  \n",
    "            tsvreader = csv.DictReader(f, delimiter=\"\\t\")  \n",
    "            for row in tsvreader:  \n",
    "                qid = row[\"query-id\"]  \n",
    "                pid = row[\"corpus-id\"]  \n",
    "                rel = int(row[\"score\"])  \n",
    "                if qid in qrels:  \n",
    "                    qrels[qid][pid] = rel  \n",
    "                else:  \n",
    "                    qrels[qid] = {pid: rel}  \n",
    "    except Exception as e:  \n",
    "        logger.error(f\"Error loading qrels file: {e}\")  \n",
    "    return qrels \n",
    " \n",
    "def load_embeddings_and_ids(embeddings_path, ids_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    ids = np.load(ids_path).astype(str)\n",
    "    return embeddings, ids\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"SlideVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/SlideVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/SlideVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/SlideVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/SlideVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-SlideVQA/qrels/slidevqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MP_DocVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/MP_DocVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/MP_DocVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/MP_DocVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/MP_DocVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-MP-DocVQA/qrels/docvqa_mp-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ArxivQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/ArxivQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/ArxivQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/ArxivQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/ArxivQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-ArxivQA/qrels/arxivqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ChartQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/ChartQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/ChartQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/ChartQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/ChartQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-ChartQA/qrels/chartqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"InfoVQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/InfoVQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/InfoVQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/InfoVQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/InfoVQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-InfoVQA/qrels/infographicsvqa-eval-qrels.tsv\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"PlotQA\",\n",
    "        \"query_embeddings_path\": \"embeddings/PlotQA_queries_with_instruction_embeddings.npy\",\n",
    "        \"query_ids_path\": \"embeddings/PlotQA_queries_query_ids.npy\",\n",
    "        \"corpus_embeddings_path\": \"embeddings/PlotQA_corpus_embeddings.npy\",\n",
    "        \"corpus_ids_path\": \"embeddings/PlotQA_corpus_corpus_ids.npy\",\n",
    "        \"qrels_path\": \"dataset/VisRAG-Ret-Test-PlotQA/qrels/plotqa-eval-qrels.tsv\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 循环评估每个数据集\n",
    "for dataset in datasets:\n",
    "    logger.info(f\"Evaluating {dataset['name']} dataset\")\n",
    "    query_embeddings, query_ids = load_embeddings_and_ids(dataset[\"query_embeddings_path\"], dataset[\"query_ids_path\"])\n",
    "    corpus_embeddings, corpus_ids = load_embeddings_and_ids(dataset[\"corpus_embeddings_path\"], dataset[\"corpus_ids_path\"])\n",
    "    qrels = load_beir_qrels(dataset[\"qrels_path\"])\n",
    "    retrieve_and_evaluate(dataset['name'], query_embeddings, query_ids, corpus_embeddings, corpus_ids, qrels)\n",
    "    logger.info('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
